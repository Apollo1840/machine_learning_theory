# Model distillation

## Knowledge distillation(KD)
Essentially is a combined loss of hard target and soft target.
The soft target is not generated by label smoothing, but inherited from a teacher model.

The student model is typically smaller than teacher, hence KD can be regarded as a model compression method.

Improves:
- FitNets, not only target, but also the intermediate features is guided.
- Attention transfer(AT), guide the student via attention map preview. (https://arxiv.org/pdf/1612.03928)
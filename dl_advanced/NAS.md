# Neural Architecture Search

## Category
- CNN NAS
- RNN NAS
- Transformer/ViT NAS
- LLM/GPT NAS
- Meta NAS
  - LEMON(https://arxiv.org/abs/2408.16168)


## Methodology

### NAS by EA

#### References
- Review: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9504890
- DeepHyperNEAT: 
- HyperNEAT ++: 
- Growth-based Evolutionary Neural Architecture Search: https://arxiv.org/pdf/2403.02667
- Evolutionary Neural Architecture Search with Generative Pre-Trained Model: https://arxiv.org/pdf/2305.05351

#### Introduction
Also called **E-NAS/EvoNAS**. Focusing on apply Evolution algorithms to search for best Neural network architecture.

NEAT(https://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) is foundation work searching the best vanilla NN archittecture.  
In NEAT, a complex network is evolved from a single cell and NN is encoded by so-called graph-encoding, 
which consists of nodes gene(list) and connection gene(adj-matrix).

In NEAT, units and connections are searched simultaneously, but other (following) works focus on one of them:
- \# layers and layers' hyper-parameters
- connections

In NEAT, model architecture is directly encoded and evoluaion process is trivial, but other (following) works focus on one of them:
- Encoding: 
  - direct: list presentation / hierarchical representation 
  - in-direct: CPPN (HyperNEAT: https://www.researchgate.net/publication/23986881_A_Hypercube-Based_Encoding_for_Evolving_Large-Scale_Neural_Networks
)
- evoluaion process:
  - layer-wise swapping
  - aging as regularization
  - mutation-only (EIC: https://arxiv.org/pdf/1703.01041)


A major challenge of ENAS is speed. Methods to speed-up the optimization are:
- proxy (a fast train/infer model to evaluate architecture)
- inheritance (weights inheritance)
- one-shot (use superNet as mother body)
- cell search (only search the building blocks)

Another challenge is to enlarge the Search Space of the ENAS, works like (https://arxiv.org/pdf/2011.10904)
explored this topic.

#### Methods

**EIC: Large-Scale Evolution of Image Classifiers**: 

With complete design of gene and mutation. 
Only mutation was considered to reproduce new individuals.

**HyperNEAT**

Use CPPN to generate ANN from subtrace (cells placed on a grid). 
CPPN use pairs of coordinates of cells as inputs, weights(after threshold) as outputs.
NEAT is used to optimize CPPN, but fitness is calculated from ANN generated by CPPN.


### NAS by RL

#### References
- RL-NAS: https://arxiv.org/pdf/1611.01578
- MnasNet: Platform-Aware Neural Architecture Search for Mobile
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

#### Introduction

**Neural Architecture Search with Reinforcement Learning**:

Use performance on validation set as rewards, and can search for LSTM cell as well.

### NAS by EA+RL
ES(Evolution Strategies) = EA + RL

### Differentiable NAS

**DARTS**: Differentiable Architecture Search 

- EfficientNet, combined with joint strategy.


### One-shot NAS
Search best architecture in a SuperNet.


**Efficient Neural Architecture Search via Parameter Sharing**
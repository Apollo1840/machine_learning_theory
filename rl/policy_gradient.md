# Policy Gradient

## Key concepts:

def. Episode: a series of (state, action, reward).

def. Policy: $ğœ‹_ğœƒ(a \mid s)$, which is the probability of taking action $a$ in state $s$, parameterized by $ğœƒ$ (weights of a neural network, for instance).


## Objective:

The goal is to maximize the **expected cumulative reward** $J(ğœƒ)$, defined as:


$$
J(\theta) = \mathbb{E}\_{\pi\_\theta} \[ \sum_{t=0}^\infty \gamma^t r_t \]
$$

$$
\sum_{t=0}^\infty \gamma^t r_t = r_0 + \gamma^1 r_1 + \gamma^2 r_2 + ... 
$$


- $r_t$: reward at time step $t$.
- $T$: episode end time.
- $\gamma$: discount factor.


## Update Rule:

Based on Policy Gradient Theorem, the gradient of $J(ğœƒ)$ with respect to $ğœƒ$ is:


$$
\nabla_{ğœƒ} J(ğœƒ) = \mathbb{E}\_{\pi\_{ğœƒ}} \[ \nabla_{ğœƒ} \log \pi_{ğœƒ}(a \mid s) \cdot G_t \]
$$


$$
G_t = \sum_{i=t}^\infty \gamma^i r_i = r_t + \gamma^{t+1} r_{t+1} + \gamma^{t+2} r_{t+2} + ... 
$$

- $G_t$: the cumulative reward (return) from time $t$.

Parameters are updated using gradient ascent:

$$
ğœƒ \gets ğœƒ + \alpha \nabla_{ğœƒ} J(ğœƒ)
$$

- $\alpha$: learning rate.

Note: the expected value $\nabla_{ğœƒ} J(ğœƒ)$ needs to be estimated. 


## Estimate the gradient:

### 1. REINFORCE Algorithm:

A Monte Carlo method that uses the sampled return $G_t$ to estimate the gradient:

$$
\nabla_{ğœƒ} J(ğœƒ) \approx \sum_t \nabla_{ğœƒ} \log \pi_{ğœƒ}(a_t \mid s_t) G_t
$$

For each episode, we will record the whole episode and calculate the discounted rewards ($G_t$) for each $t$,
as well as $\pi_{ğœƒ}(a_t \mid s_t)$:

$$
 \{(s_t, a_t, r_t)\} -> \{(\pi_{ğœƒ}(a_t \mid s_t), G_t)\}
$$

### 2. Actor-Critic Methods:

Combines policy gradients (actor) with value-based methods (critic). The critic estimates a value function (e.g., $V(s)$ ), which reduces the variance of the gradient estimate.

Introduces the advantage function $A_\theta(s, a) = Q_\theta(s, a) - V_\theta(s)$ to improve stability:

$$
\nabla_{ğœƒ} J(ğœƒ) \propto \nabla_{ğœƒ} \log \pi_{ğœƒ}(a \mid s) A_\theta(s, a)
$$


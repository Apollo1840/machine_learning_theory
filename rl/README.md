# Reinforcement learning

references:
- (My onenote): https://onedrive.live.com/edit.aspx?resid=BEF76BD482A6B496!16288&migratedtospo=true&wd=target%28%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.one%7Cca12a274-13c7-4b7f-a61a-5d791241a9ae%2Flink%7C35812e9f-a465-4bcc-8efd-5537a1f5da01%2F%29&wdorigin=NavigationUrl
- (My implementation): https://github.com/Apollo1840/Reinforcement-Learning-Tech

---

Reinforcement learning is a type of machine learning where 

    an agent learns to make decisions to achieve a specific goal. 

The agent learns through receiving **feedback** from its actions within an environment in the form of rewards (or penalties).

## Key concepts:

- def. Episode: Time period of an agents interacting with environment until termination.
- def. Trajectory: Series of `(state, action, reward)` pairs collected from one episode, ie. $\{(s_t, a_t, r_t)\}_t$.
- def. Policy: $ùúã_ùúÉ(a \mid s)$, which is the probability distribution of action $a$ at state $s$, parameterized by $ùúÉ$ (weights of a neural network, for instance). 
  Note that this is **NOT** a probability value of `(s, a)` pair.

## Category
From the perspective of the learning schema, reinforcement learning (RL) can be categorized into two types:
- Online RL
- Offline RL

From the perspective of environment modeling, reinforcement learning (RL) can be categorized into two types:
- Model-based RL
- Model-free RL

From the perspective of policy interaction with data, reinforcement learning (RL) can be categorized into two types:
- On-policy RL
- Off-policy RL

### Online RL
In online RL, the agent interacts with a **LIVE** environment, where it can query the current reward and the next state based on its actions. 
During or after experiencing an **episode**, the model updates its weights and improves its strategy through exploration. 
The learning process happens iteratively as the agent interacts with the environment. 

Methods:
- Q-Learning
- Deep Q-Networks (DQN)
- Proximal Policy Optimization (PPO) 

### Offline RL
In offline RL, the agent does not interact with a live environment. 
Instead, it learns from a **FIXED** dataset consisting of sequences of `(state, action, reward)` collected beforehand. 
The RL algorithm trains the model solely using this static dataset. 
This approach is particularly useful in scenarios where environment interactions are costly, dangerous, or impractical. 

Methods:
- Batch Q-Learning
- Implicit Q-Learning (IQL) 
- Conservative Q-Learning (CQL) 


### Model-based RL
reference: https://onedrive.live.com/edit.aspx?resid=BEF76BD482A6B496!16288&migratedtospo=true&wd=target%28%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.one%7Cca12a274-13c7-4b7f-a61a-5d791241a9ae%2FLesson%203%20DP%7C9e0cf76e-a2f9-4afd-acdd-98e1522f9350%2F%29&wdorigin=NavigationUrl

In model-based RL, the environment is assumed to behave like a parameterized stochastic system, 
and the goal is to learn the parameters of this environment model.
Once the model of the environment is learned, it can be used to derive the optimal policy, which specifies the best actions to take in each state to achieve a desired outcome.

The environment is often modeled as a Markov Decision Process (**MDP**), where:

- Each state is associated with a reward.
- The objective is to determine a policy (a sequence of actions over state) that maximizes the expected discounted rewards over time.
This approach explicitly uses the learned environment model to simulate or plan future actions and outcomes.

### Model-free RL
In contrast, model-free RL does not attempt to explicitly model the environment. Instead, it directly learns the optimal policy or value function based on the agent's interactions with the environment. The agent explores the environment, collects experiences, and adjusts its strategy to maximize the cumulative reward.

Model-free RL methods are further divided into:

- **Value-based methods**: Learn a value function `Q(s, a)` to guide action selection (e.g., Q-Learning).
- **Policy-based methods**: Directly learn a policy `pi(s)` that maps states to actions (e.g., REINFORCE).
- **Actor-Critic methods**: Combine value-based and policy-based approaches for efficiency and stability. 
  Actor corresponds to `pi(s)` and Critic corresponds to `V(s)` which used to estimate advantage (`Q(s, a)-V(s)`) better, 
  so `pi(s)` can be optimized.

Compared to model-based RL, model-free methods are generally simpler to implement and do not rely on accurate modeling of the environment. However, they often require more data and computational resources to converge to an optimal policy.


### On-policy RL
In on-policy RL, the agent learns using data generated by the current policy it is optimizing. 
This means the policy that is being evaluated and improved is the same policy used to interact with the environment and collect data.
On-policy methods often emphasize stability during training and are better suited for environments where exploration needs to be guided by the current policy. 
However, they can be sample-inefficient as they discard past data that no longer matches the current policy. 

Methods:
- REINFORCE
- Proximal Policy Optimization (PPO)

### Off-policy RL
In off-policy RL, the agent learns using data that may have been collected by a different policy or even a random behavior policy. 
This allows off-policy methods to reuse historical data, making them more sample-efficient than on-policy methods. 
The agent evaluates and improves the **target policy** independently of the **behavior policy** that generated the data.
This flexibility makes off-policy RL more versatile but can lead to challenges like stability issues and the need for careful exploration-exploitation balancing. 

Methods:
- Q-Learning
- Deep Q-Networks (DQN)
- Soft Actor-Critic (SAC)

  
## Basic Methods

### MC & TD
see `mc_td.md`.

### SARSA & Q-learning
see `./sarsa_qlearning.md`.

- state space: finite
- action space: finite

### DQN
see `DQN.md`.

- state space: $\infty$
- action space: finite

### PG-AC
see `policy_gradient.md`.

- state space: $\infty$
- action space: $\infty$
# Proximal Policy Optimization

PPO is based on Policy Gradient Optimization(PGO), please refer to `policy_gradient.md` for details.
